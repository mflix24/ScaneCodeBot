{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-00 : importing the libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and packages\n",
    "import os\n",
    "# we need to import Report for cloning any git hub repository\n",
    "from git import Repo\n",
    "\n",
    "# for context aware splitting we nned to import Language\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "# since we cloned any repo we need to import GenericLoader for loading the documents\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "\n",
    "# RecursiveCharacterTextSplitter needed for splitting the document into chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# we need to import OpenAIEmbeddings for embedding the chunking document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# we need to import Chroma for string embedding data\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# selecting our models through ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# for memorize the Semmary\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# chaining all the things thar done on earlier lines\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-01 : cloning the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating folder on current directory\n",
    "# this folder contains all the files and folders which we are going to clone from github repository\n",
    "! mkdir files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloning the repository\n",
    "repo_path = \"files/\"\n",
    "Repo.clone_from(\"https://github.com/entbappy/End-to-end-ML-Project-Implementation\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-02 : Load the repository through the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an obejct of GenericLoader() class and through this class \n",
    "# call the from_filesystem() function with few parameters\n",
    "# here loader is the object\n",
    "repo_path = \"files/\"\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path+'/src/mlProject',\n",
    "    glob = \"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through the loader object, we can call its other functions like load() function\n",
    "# loader is the object of GenericLoader class.\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing few attribute\n",
    "documents[0].page_content\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-03 : splitting the documents into chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an obejct of RecursiveCharacterTextSplitter() class and through this class \n",
    "# call the from_language() function with few parameters\n",
    "# here documents_splitter is the object\n",
    "# here we just creating an object\n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language = Language.PYTHON,\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# through the documents_splitter object, we can call its other functions like split_documents() function\n",
    "# documents_splitter is the object of RecursiveCharacterTextSplitter class.\n",
    "# here we split the whole document into chunk through the object we created here as documents_splitter\n",
    "# saving the whole chunked data into a varibale called as chunked_texts\n",
    "chunked_texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-04 : Setting the API KEY for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing few things\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Key from .env file\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-05 : loading the embbing model through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object of OpenAIEmbeddings class\n",
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an obejct of Chroma() class and through this class \n",
    "# call the from_documents() function with few parameters\n",
    "# here vectordb is the object\n",
    "# here we pass the whole chunked_texts and the embeddings model here we are using openai embeddings\n",
    "vectordb = Chroma.from_documents(\n",
    "    chunked_texts, \n",
    "    embedding=embeddings, \n",
    "    persist_directory='./data'\n",
    "    )\n",
    "\n",
    "# through the object we can call other functions like persist function\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-06 : loading the model that are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object of ChatOpenAI class \n",
    "# By default it uses the model called as ''\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-07 : Setting Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object of ConversationSummaryMemory function andpassing few parameters\n",
    "# loading the model as llm\n",
    "# its for memorising when we combine all the things\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm, \n",
    "    memory_key = \"chat_history\", \n",
    "    return_messages=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-08 : Chaining all the things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an obejct of ConversationalRetrievalChain() class and through this class \n",
    "# call the from_llm() function with few parameters\n",
    "# here qa is the object\n",
    "# here we just creating an object\n",
    "model_object = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    retriever=vectordb.as_retriever(\n",
    "        search_type=\"mmr\", \n",
    "        search_kwargs={\"k\":3}),  \n",
    "    memory=memory\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-09 : prompt designing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt designing here\n",
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-10 : Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model ging the result\n",
    "result = model_object(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets converting this notebook into modular coding :\n",
    "Modular coding is nothing but the codes reusability. Sometimes we need the same code in different functionabilities. For reusing the same code we always follow the modular codding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
